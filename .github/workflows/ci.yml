name: Tests

on:
  push:
    branches:
      - main
  pull_request:
    types:
      - opened
      - synchronize
      - labeled
      - unlabeled
    paths-ignore:
      - "packages/docs/**"

env:
  LLM_MAX_MS: "15000"
  EVAL_ENV: "browserbase"
  EVAL_MODELS: "openai/gpt-4.1,google/gemini-2.0-flash,anthropic/claude-haiku-4-5"
  EVAL_AGENT_MODELS: "computer-use-preview-2025-03-11,claude-3-7-sonnet-latest"
  EVAL_CATEGORIES: "observe,act,combination,extract,targeted_extract,agent"
  EVAL_MAX_CONCURRENCY: 25
  EVAL_TRIAL_COUNT: 3
  EVAL_AGENT_MAX_CONCURRENCY: 30
  EVAL_AGENT_TRIAL_COUNT: 2
  LOCAL_SESSION_LIMIT_PER_E2E_TEST: 2
  BROWSERBASE_SESSION_LIMIT_PER_E2E_TEST: 3
  BROWSERBASE_REGION_DISTRIBUTION: "us-west-2=30,us-east-1=30,eu-central-1=20,ap-southeast-1=20"  # percentage of load for each region when running e2e tests against prod
  CHROME_PATH: /usr/bin/chromium # GitHub Actions runners ship with Chromium by default
  BROWSERBASE_CDP_CONNECT_MAX_MS: "10000"
  BROWSERBASE_SESSION_CREATE_MAX_MS: "10000"
  TURBO_TELEMETRY_DISABLED: 1

concurrency:
  group: ${{ github.ref }}
  cancel-in-progress: true

jobs:
  determine-changes:
    runs-on: ubuntu-latest
    outputs:
      core: ${{ steps.filter.outputs.core }}
      evals: ${{ steps.filter.outputs.evals }}
      server: ${{ steps.filter.outputs.server }}
      docs-only: ${{ steps.filter.outputs.docs-only }}
    steps:
      - name: Check out repository code
        uses: actions/checkout@v4

      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            core:
              - '.github/workflows/ci.yml'
              - 'packages/core/**'
              - 'package.json'
              - 'pnpm-lock.yaml'
              - 'turbo.json'
            evals:
              - 'packages/evals/**'
              - 'package.json'
              - 'pnpm-lock.yaml'
            server:
              - 'packages/server/**'
              - 'packages/core/**'
              - 'package.json'
              - 'pnpm-lock.yaml'
              - 'pnpm-workspace.yaml'
              - '.github/workflows/ci.yml'
            docs-only:
              - '**/*.md'
              - 'examples/**'
              - '!packages/**/*.md'

  determine-evals:
    needs: [determine-changes]
    runs-on: ubuntu-latest
    outputs:
      skip-all-evals: ${{ steps.check-labels.outputs.skip-all-evals }}
      run-regression: ${{ steps.check-labels.outputs.run-regression }}
      run-combination: ${{ steps.check-labels.outputs.run-combination }}
      run-extract: ${{ steps.check-labels.outputs.run-extract }}
      run-act: ${{ steps.check-labels.outputs.run-act }}
      run-observe: ${{ steps.check-labels.outputs.run-observe }}
      run-targeted-extract: ${{ steps.check-labels.outputs.run-targeted-extract }}
      run-agent: ${{ steps.check-labels.outputs.run-agent }}
    steps:
      - id: check-labels
        run: |
          # Check if skip-evals label is present
          if [[ "${{ contains(github.event.pull_request.labels.*.name, 'skip-evals') }}" == "true" ]]; then
            echo "skip-evals label found - skipping all evals"
            echo "skip-all-evals=true" >> $GITHUB_OUTPUT
            echo "run-regression=false" >> $GITHUB_OUTPUT
            echo "run-combination=false" >> $GITHUB_OUTPUT
            echo "run-extract=false" >> $GITHUB_OUTPUT
            echo "run-act=false" >> $GITHUB_OUTPUT
            echo "run-observe=false" >> $GITHUB_OUTPUT
            echo "run-targeted-extract=false" >> $GITHUB_OUTPUT
            echo "run-agent=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Skip evals if only docs/examples changed (and not on main)
          if [[ "${{ needs.determine-changes.outputs.docs-only }}" == "true" && "${{ needs.determine-changes.outputs.core }}" == "false" && "${{ needs.determine-changes.outputs.evals }}" == "false" && "${{ github.ref }}" != "refs/heads/main" ]]; then
            echo "Only docs/examples changed - skipping evals"
            echo "skip-all-evals=true" >> $GITHUB_OUTPUT
            echo "run-regression=false" >> $GITHUB_OUTPUT
            echo "run-combination=false" >> $GITHUB_OUTPUT
            echo "run-extract=false" >> $GITHUB_OUTPUT
            echo "run-act=false" >> $GITHUB_OUTPUT
            echo "run-observe=false" >> $GITHUB_OUTPUT
            echo "run-targeted-extract=false" >> $GITHUB_OUTPUT
            echo "run-agent=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Default to running all tests on main branch
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "Running all tests for main branch"
            echo "skip-all-evals=false" >> $GITHUB_OUTPUT
            echo "run-regression=true" >> $GITHUB_OUTPUT
            echo "run-combination=true" >> $GITHUB_OUTPUT
            echo "run-extract=true" >> $GITHUB_OUTPUT
            echo "run-act=true" >> $GITHUB_OUTPUT
            echo "run-observe=true" >> $GITHUB_OUTPUT
            echo "run-targeted-extract=true" >> $GITHUB_OUTPUT
            echo "run-agent=true" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Check for skip-regression-evals label
          if [[ "${{ contains(github.event.pull_request.labels.*.name, 'skip-regression-evals') }}" == "true" ]]; then
            echo "skip-regression-evals label found - regression evals will be skipped"
            echo "run-regression=false" >> $GITHUB_OUTPUT
          else
            echo "Regression evals will run by default"
            echo "run-regression=true" >> $GITHUB_OUTPUT
          fi

          # Check for specific labels
          echo "skip-all-evals=false" >> $GITHUB_OUTPUT
          echo "run-combination=${{ contains(github.event.pull_request.labels.*.name, 'combination') }}" >> $GITHUB_OUTPUT
          echo "run-extract=${{ contains(github.event.pull_request.labels.*.name, 'extract') }}" >> $GITHUB_OUTPUT
          echo "run-act=${{ contains(github.event.pull_request.labels.*.name, 'act') }}" >> $GITHUB_OUTPUT
          echo "run-observe=${{ contains(github.event.pull_request.labels.*.name, 'observe') }}" >> $GITHUB_OUTPUT
          echo "run-targeted-extract=${{ contains(github.event.pull_request.labels.*.name, 'targeted-extract') }}" >> $GITHUB_OUTPUT
          echo "run-agent=${{ contains(github.event.pull_request.labels.*.name, 'agent') }}" >> $GITHUB_OUTPUT
      
  run-build:
    name: Lint & Build
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository code
        uses: actions/checkout@v4

      - uses: ./.github/actions/setup-node-pnpm-turbo
        with:
          use-prebuilt-artifacts: "false"

      - name: Run Lint
        run: pnpm run lint

      - name: Run Build
        run: pnpm run build

      - name: Save Turbo cache
        if: always()
        uses: actions/cache/save@v4
        with:
          path: .turbo
          key: ${{ runner.os }}-turbo-${{ github.sha }}

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build-artifacts
          path: |
            package.json # Placeholder to anchor artifact root at repo root. do not remove or artifacts paths will all change to common ancestor packages/
            packages/core/dist/**
            packages/core/lib/version.ts
            packages/core/lib/dom/build/**
            packages/core/lib/v3/dom/build/**
            packages/server/dist/**
            packages/server/openapi.v3.yaml
          retention-days: 1

  discover-core-tests:
    runs-on: ubuntu-latest
    needs: [determine-changes]
    if: needs.determine-changes.outputs.core == 'true'
    outputs:
      core-tests: ${{ steps.set-matrix.outputs.core-tests }}
      has-core-tests: ${{ steps.set-matrix.outputs.has-core-tests }}

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Discover core test files
        id: set-matrix
        run: |
          core_tests=$(find packages/core/tests -type f -name "*.test.ts" 2>/dev/null | sort || true)

          core_json="["
          first=true
          for test_file in $core_tests; do
            if [ "$first" = true ]; then
              first=false
            else
              core_json+=","
            fi
            name=$(echo "$test_file" | sed 's|^packages/core/tests/||' | sed 's|\.test\.ts$||')
            core_json+="{\"path\":\"$test_file\",\"name\":\"$name\"}"
          done
          core_json+="]"

          echo "core-tests=$core_json" >> $GITHUB_OUTPUT

          if [ "$core_json" = "[]" ]; then
            echo "has-core-tests=false" >> $GITHUB_OUTPUT
          else
            echo "has-core-tests=true" >> $GITHUB_OUTPUT
          fi

          echo "Found core tests: $core_json"

  core-unit-tests:
    name: core/${{ matrix.test.name }}
    runs-on: ubuntu-latest
    needs: [run-build, discover-core-tests]
    if: needs.discover-core-tests.outputs.has-core-tests == 'true'
    env:
      NODE_V8_COVERAGE: ${{ github.workspace }}/coverage/v8/core-unit/${{ matrix.test.name }}

    strategy:
      fail-fast: true
      max-parallel: 100
      matrix:
        test: ${{ fromJson(needs.discover-core-tests.outputs.core-tests) }}

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - uses: ./.github/actions/setup-node-pnpm-turbo

      - name: Run Vitest - ${{ matrix.test.name }}
        run: |
          test_path="${{ matrix.test.path }}"
          rel_path="${test_path#packages/core/}"
          pnpm --filter @browserbasehq/stagehand exec vitest run \
            --config vitest.config.ts \
            --reporter=default \
            --reporter=junit \
            --outputFile.junit="${{ github.workspace }}/ctrf/vitest-${{ matrix.test.name }}.xml" \
            --coverage \
            --coverage.reportsDirectory="${{ github.workspace }}/coverage/vitest/${{ matrix.test.name }}" \
            --coverage.reporter=lcov \
            --coverage.reporter=text-summary \
            "$rel_path"

      - name: Upload Vitest coverage - ${{ matrix.test.name }}
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: vitest-coverage-${{ matrix.test.name }}
          path: coverage/vitest/${{ matrix.test.name }}

      - uses: ./.github/actions/publish-ctrf-report
        if: always()
        with:
          junit-path: ${{ github.workspace }}/ctrf/vitest-${{ matrix.test.name }}.xml
          tool: vitest
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - uses: ./.github/actions/upload-v8-coverage
        if: always()
        with:
          name: coverage-v8-core-unit-${{ matrix.test.name }}

  discover-server-tests:
    runs-on: ubuntu-latest
    needs: [determine-changes]
    if: needs.determine-changes.outputs.server == 'true'
    outputs:
      unit-tests: ${{ steps.set-matrix.outputs.unit-tests }}
      integration-tests: ${{ steps.set-matrix.outputs.integration-tests }}
      has-unit-tests: ${{ steps.set-matrix.outputs.has-unit-tests }}
      has-integration-tests: ${{ steps.set-matrix.outputs.has-integration-tests }}

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Discover server test files
        id: set-matrix
        run: |
          cd packages/server

          unit_tests=$(find ./test/unit -type f -name "*.test.ts" 2>/dev/null | sort || true)
          integration_tests=$(find ./test/integration -type f -name "*.test.ts" 2>/dev/null | sort || true)

          unit_json="["
          first=true
          for test_file in $unit_tests; do
            if [ "$first" = true ]; then
              first=false
            else
              unit_json+=","
            fi
            name=$(basename "$test_file" .test.ts)
            unit_json+="{\"path\":\"$test_file\",\"name\":\"$name\"}"
          done
          unit_json+="]"

          int_json="["
          first=true
          for test_file in $integration_tests; do
            if [ "$first" = true ]; then
              first=false
            else
              int_json+=","
            fi
            name=$(echo "$test_file" | sed 's|^\./test/integration/||' | sed 's|\.test\.ts$||')
            int_json+="{\"path\":\"$test_file\",\"name\":\"$name\"}"
          done
          int_json+="]"

          echo "unit-tests=$unit_json" >> $GITHUB_OUTPUT
          echo "integration-tests=$int_json" >> $GITHUB_OUTPUT

          if [ "$unit_json" = "[]" ]; then
            echo "has-unit-tests=false" >> $GITHUB_OUTPUT
          else
            echo "has-unit-tests=true" >> $GITHUB_OUTPUT
          fi

          if [ "$int_json" = "[]" ]; then
            echo "has-integration-tests=false" >> $GITHUB_OUTPUT
          else
            echo "has-integration-tests=true" >> $GITHUB_OUTPUT
          fi

          echo "Found server unit tests: $unit_json"
          echo "Found server integration tests: $int_json"

  build-server-sea:
    name: Build SEA binary
    needs: [discover-server-tests]
    if: needs.discover-server-tests.outputs.has-integration-tests == 'true'
    uses: ./.github/workflows/stagehand-server-sea-build.yml
    with:
      matrix: |
        [
          {"os":"ubuntu-latest","platform":"linux","arch":"x64","binary_name":"stagehand-server-linux-x64"}
        ]

  server-unit-tests:
    name: server/unit/${{ matrix.test.name }}
    runs-on: ubuntu-latest
    needs: [discover-server-tests]
    if: needs.discover-server-tests.outputs.has-unit-tests == 'true'
    env:
      NODE_V8_COVERAGE: ${{ github.workspace }}/coverage/v8/server-unit/${{ matrix.test.name }}

    strategy:
      fail-fast: false
      matrix:
        test: ${{ fromJson(needs.discover-server-tests.outputs.unit-tests) }}

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - uses: ./.github/actions/setup-node-pnpm-turbo

      - name: Run server unit test - ${{ matrix.test.name }}
        env:
          NODE_TEST_REPORTER: junit
          NODE_TEST_REPORTER_DESTINATION: ${{ github.workspace }}/ctrf/node-test-unit-${{ matrix.test.name }}.xml
        run: |
          cd packages/server
          pnpm run node:test "${{ matrix.test.path }}"

      - uses: ./.github/actions/publish-ctrf-report
        if: always()
        with:
          junit-path: ${{ github.workspace }}/ctrf/node-test-unit-${{ matrix.test.name }}.xml
          tool: node-test
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - uses: ./.github/actions/upload-v8-coverage
        if: always()
        with:
          name: coverage-v8-server-unit-${{ matrix.test.name }}

  server-integration-tests:
    name: server/integration/${{ matrix.test.name }}
    runs-on: ubuntu-latest
    needs: [build-server-sea, discover-server-tests]
    if: needs.discover-server-tests.outputs.has-integration-tests == 'true'

    strategy:
      fail-fast: false
      matrix:
        test: ${{ fromJson(needs.discover-server-tests.outputs.integration-tests) }}

    env:
      BB_ENV: local
      PORT: "3107"
      STAGEHAND_API_URL: http://127.0.0.1:3107
      NODE_V8_COVERAGE: ${{ github.workspace }}/coverage/v8/server-integration/${{ matrix.test.name }}
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - uses: ./.github/actions/setup-node-pnpm-turbo
        with:
          use-prebuilt-artifacts: "false"

      - name: Download SEA binary
        uses: actions/download-artifact@v4
        with:
          name: stagehand-server-linux-x64
          path: packages/server/dist/sea

      - name: Ensure SEA binary is executable
        run: |
          chmod +x packages/server/dist/sea/stagehand-server-linux-x64

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/ms-playwright
          key: ${{ runner.os }}-${{ runner.arch }}-playwright-${{ hashFiles('pnpm-lock.yaml') }}
          restore-keys: |
            ${{ runner.os }}-${{ runner.arch }}-playwright-

      - name: Install Playwright browsers
        run: |
          pnpm dlx playwright install --with-deps chromium

      - name: Start stagehand server (SEA binary)
        env:
          NODE_ENV: production
        run: |
          packages/server/dist/sea/stagehand-server-linux-x64 &
          echo $! > /tmp/server.pid

          echo "Waiting for server to start..."
          for i in {1..30}; do
            if curl -s http://127.0.0.1:3107/healthz > /dev/null 2>&1; then
              echo "Server is ready!"
              break
            fi
            if [ $i -eq 30 ]; then
              echo "Server failed to start within 30 seconds"
              exit 1
            fi
            sleep 1
          done

      - name: Run server integration test - ${{ matrix.test.name }}
        env:
          NODE_TEST_REPORTER: junit
          NODE_TEST_REPORTER_DESTINATION: ${{ github.workspace }}/ctrf/node-test-integration-${{ matrix.test.name }}.xml
          NODE_TEST_EXTRA_ARGS: --experimental-test-coverage
        run: |
          cd packages/server
          pnpm run node:test "${{ matrix.test.path }}"

      - uses: ./.github/actions/publish-ctrf-report
        if: always()
        with:
          junit-path: ${{ github.workspace }}/ctrf/node-test-integration-${{ matrix.test.name }}.xml
          tool: node-test
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - uses: ./.github/actions/upload-v8-coverage
        if: always()
        with:
          name: coverage-v8-server-integration-${{ matrix.test.name }}

      - name: Stop server
        if: always()
        run: |
          if [ -f /tmp/server.pid ]; then
            kill $(cat /tmp/server.pid) 2>/dev/null || true
          fi

  discover-e2e-tests:
    runs-on: ubuntu-latest
    needs: [determine-changes]
    if: needs.determine-changes.outputs.core == 'true'
    outputs:
      e2e-tests: ${{ steps.set-matrix.outputs.e2e-tests }}
      has-e2e-tests: ${{ steps.set-matrix.outputs.has-e2e-tests }}

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Discover e2e test files
        id: set-matrix
        run: |
          e2e_tests=$(find packages/core/lib/v3/tests -type f -name "*.spec.ts" 2>/dev/null | sort || true)

          e2e_json="["
          first=true
          for test_file in $e2e_tests; do
            if [ "$first" = true ]; then
              first=false
            else
              e2e_json+=","
            fi
            name=$(echo "$test_file" | sed 's|^packages/core/lib/v3/tests/||' | sed 's|\.spec\.ts$||')
            e2e_json+="{\"path\":\"$test_file\",\"name\":\"$name\"}"
          done
          e2e_json+="]"

          echo "e2e-tests=$e2e_json" >> $GITHUB_OUTPUT

          if [ "$e2e_json" = "[]" ]; then
            echo "has-e2e-tests=false" >> $GITHUB_OUTPUT
          else
            echo "has-e2e-tests=true" >> $GITHUB_OUTPUT
          fi

          echo "Found e2e tests: $e2e_json"

  run-e2e-local-tests:
    name: e2e/local/${{ matrix.test.name }}
    needs: [run-build, discover-e2e-tests]
    runs-on: ubuntu-latest
    timeout-minutes: 50
    if: >
      needs.discover-e2e-tests.outputs.has-e2e-tests == 'true' &&
      (github.event_name == 'push' ||
      (github.event_name == 'pull_request' && github.event.pull_request.head.repo.full_name == github.repository))
    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      GOOGLE_GENERATIVE_AI_API_KEY: ${{ secrets.GOOGLE_GENERATIVE_AI_API_KEY }}
      BROWSERBASE_API_KEY: ${{ secrets.BROWSERBASE_API_KEY }}
      BROWSERBASE_PROJECT_ID: ${{ secrets.BROWSERBASE_PROJECT_ID }}
      HEADLESS: true
      CTRF_JUNIT_PATH: ${{ github.workspace }}/ctrf/playwright-local-${{ matrix.test.name }}.xml
      NODE_V8_COVERAGE: ${{ github.workspace }}/coverage/v8/e2e-local/${{ matrix.test.name }}
    strategy:
      fail-fast: true
      max-parallel: 50
      matrix:
        test: ${{ fromJson(needs.discover-e2e-tests.outputs.e2e-tests) }}
    steps:
      - name: Check out repository code
        uses: actions/checkout@v4

      - uses: ./.github/actions/setup-node-pnpm-turbo

      - uses: ./.github/actions/verify-chromium-launch

      - name: Run local E2E Tests - ${{ matrix.test.name }}
        run: |
          test_path="${{ matrix.test.path }}"
          rel_path="${test_path#packages/core/}"
          pnpm exec turbo run e2e:local --only --log-order=stream \
            --filter @browserbasehq/stagehand -- "$rel_path"

      - uses: ./.github/actions/publish-ctrf-report
        if: always()
        with:
          junit-path: ${{ github.workspace }}/ctrf/playwright-local-${{ matrix.test.name }}.xml
          tool: playwright
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - uses: ./.github/actions/upload-v8-coverage
        if: always()
        with:
          name: coverage-v8-e2e-local-${{ matrix.test.name }}

  run-e2e-bb-tests:
    name: e2e/bb/${{ matrix.test.name }}
    needs: [run-build, discover-e2e-tests]
    runs-on: ubuntu-latest
    timeout-minutes: 50
    if: >
      needs.discover-e2e-tests.outputs.has-e2e-tests == 'true' &&
      (github.event_name == 'push' ||
      (github.event_name == 'pull_request' && github.event.pull_request.head.repo.full_name == github.repository))
    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      GOOGLE_GENERATIVE_AI_API_KEY: ${{ secrets.GOOGLE_GENERATIVE_AI_API_KEY }}
      BROWSERBASE_API_KEY: ${{ secrets.BROWSERBASE_API_KEY }}
      BROWSERBASE_PROJECT_ID: ${{ secrets.BROWSERBASE_PROJECT_ID }}
      HEADLESS: true
      CTRF_JUNIT_PATH: ${{ github.workspace }}/ctrf/playwright-bb-${{ matrix.test.name }}.xml
      NODE_V8_COVERAGE: ${{ github.workspace }}/coverage/v8/e2e-bb/${{ matrix.test.name }}
    strategy:
      fail-fast: true
      max-parallel: 100
      matrix:
        test: ${{ fromJson(needs.discover-e2e-tests.outputs.e2e-tests) }}
    steps:
      - name: Check out repository code
        uses: actions/checkout@v4

      - uses: ./.github/actions/setup-node-pnpm-turbo

      - name: Select Browserbase region
        uses: ./.github/actions/select-browserbase-region
        with:
          distribution: ${{ env.BROWSERBASE_REGION_DISTRIBUTION }}

      - name: Run E2E Tests (browserbase) - ${{ matrix.test.name }}
        run: |
          test_path="${{ matrix.test.path }}"
          rel_path="${test_path#packages/core/}"
          pnpm exec turbo run e2e:bb --only --log-order=stream \
            --filter @browserbasehq/stagehand -- "$rel_path"

      - uses: ./.github/actions/publish-ctrf-report
        if: always()
        with:
          junit-path: ${{ github.workspace }}/ctrf/playwright-bb-${{ matrix.test.name }}.xml
          tool: playwright
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - uses: ./.github/actions/upload-v8-coverage
        if: always()
        with:
          name: coverage-v8-e2e-bb-${{ matrix.test.name }}

  run-regression-evals:
    name: evals/regression
    needs: [run-build, determine-evals, run-e2e-bb-tests]
    if: >-
      ${{
        always() &&
        needs.run-build.result == 'success' &&
        needs.determine-evals.result == 'success' &&
        needs.run-e2e-bb-tests.result != 'failure' &&
        needs.run-e2e-bb-tests.result != 'cancelled' &&
        needs.determine-evals.outputs.skip-all-evals != 'true' &&
        needs.determine-evals.outputs.run-regression == 'true'
      }}
    runs-on: ubuntu-latest
    timeout-minutes: 9
    outputs:
      regression_score: ${{ steps.set-regression-score.outputs.regression_score }}
    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      GOOGLE_GENERATIVE_AI_API_KEY: ${{ secrets.GOOGLE_GENERATIVE_AI_API_KEY }}
      BRAINTRUST_API_KEY: ${{ secrets.BRAINTRUST_API_KEY }}
      BROWSERBASE_API_KEY: ${{ secrets.BROWSERBASE_API_KEY }}
      BROWSERBASE_PROJECT_ID: ${{ secrets.BROWSERBASE_PROJECT_ID }}
      HEADLESS: true
      NODE_V8_COVERAGE: ${{ github.workspace }}/coverage/v8/evals/regression
    steps:
      - name: Check out repository code
        uses: actions/checkout@v4

      - uses: ./.github/actions/setup-node-pnpm-turbo

      - name: Select Browserbase region
        uses: ./.github/actions/select-browserbase-region
        with:
          distribution: ${{ env.BROWSERBASE_REGION_DISTRIBUTION }}

      - name: Run Regression Evals
        run: pnpm run evals category regression trials=2 concurrency=${EVAL_MAX_CONCURRENCY} env=BROWSERBASE

      - name: Log Regression Evals Performance
        id: set-regression-score
        run: |
          experimentName=$(jq -r '.experimentName' eval-summary.json)
          echo "View results at https://www.braintrust.dev/app/Browserbase/p/stagehand/experiments/${experimentName}"
          if [ -f eval-summary.json ]; then
            regression_score=$(jq '.categories.regression' eval-summary.json)
            echo "Regression category score: $regression_score%"
            echo "regression_score=$regression_score" >> $GITHUB_OUTPUT
            if (( $(echo "$regression_score < 90" | bc -l) )); then
              echo "Regression category score is below 90%. Failing CI."
              exit 1
            fi
          else
            echo "Eval summary not found for regression category. Failing CI."
            exit 1
          fi

      - uses: ./.github/actions/write-evals-ctrf
        if: always()
        with:
          category: regression

      - uses: ./.github/actions/publish-ctrf-report
        if: always()
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - uses: ./.github/actions/upload-v8-coverage
        if: always()
        with:
          name: coverage-v8-evals-regression

  run-combination-evals:
    name: evals/combination
    needs: [run-build, determine-evals, run-e2e-bb-tests, run-regression-evals]
    if: >-
      ${{
        always() &&
        needs.run-build.result == 'success' &&
        needs.determine-evals.result == 'success' &&
        needs.run-e2e-bb-tests.result != 'failure' &&
        needs.run-e2e-bb-tests.result != 'cancelled' &&
        needs.run-regression-evals.result != 'failure' &&
        needs.run-regression-evals.result != 'cancelled' &&
        needs.determine-evals.outputs.skip-all-evals != 'true'
      }}
    runs-on: ubuntu-latest
    timeout-minutes: 40
    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      GOOGLE_GENERATIVE_AI_API_KEY: ${{ secrets.GOOGLE_GENERATIVE_AI_API_KEY }}
      BRAINTRUST_API_KEY: ${{ secrets.BRAINTRUST_API_KEY }}
      BROWSERBASE_API_KEY: ${{ secrets.BROWSERBASE_API_KEY }}
      BROWSERBASE_PROJECT_ID: ${{ secrets.BROWSERBASE_PROJECT_ID }}
      HEADLESS: true
      NODE_V8_COVERAGE: ${{ github.workspace }}/coverage/v8/evals/combination
    steps:
      - name: Check out repository code
        uses: actions/checkout@v4

      - name: Check for 'combination' label
        id: label-check
        run: |
          if [ "${{ needs.determine-evals.outputs.run-combination }}" != "true" ]; then
            echo "has_label=false" >> $GITHUB_OUTPUT
            echo "No label for COMBINATION. Exiting with success."
          else
            echo "has_label=true" >> $GITHUB_OUTPUT
          fi

      - uses: ./.github/actions/setup-node-pnpm-turbo
        if: needs.determine-evals.outputs.run-combination == 'true'

      - name: Select Browserbase region
        if: needs.determine-evals.outputs.run-combination == 'true'
        uses: ./.github/actions/select-browserbase-region
        with:
          distribution: ${{ env.BROWSERBASE_REGION_DISTRIBUTION }}

      - name: Run Combination Evals
        if: needs.determine-evals.outputs.run-combination == 'true'
        run: pnpm run evals category combination

      - name: Log Combination Evals Performance
        if: needs.determine-evals.outputs.run-combination == 'true'
        run: |
          experimentName=$(jq -r '.experimentName' eval-summary.json)
          echo "View results at https://www.braintrust.dev/app/Browserbase/p/stagehand/experiments/${experimentName}"
          if [ -f eval-summary.json ]; then
            combination_score=$(jq '.categories.combination' eval-summary.json)
            echo "Combination category score: $combination_score%"
            exit 0
          else
            echo "Eval summary not found for combination category. Failing CI."
            exit 1
          fi

      - uses: ./.github/actions/write-evals-ctrf
        if: always() && needs.determine-evals.outputs.run-combination == 'true'
        with:
          category: combination

      - uses: ./.github/actions/publish-ctrf-report
        if: always() && needs.determine-evals.outputs.run-combination == 'true'
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - uses: ./.github/actions/upload-v8-coverage
        if: always() && needs.determine-evals.outputs.run-combination == 'true'
        with:
          name: coverage-v8-evals-combination

  run-act-evals:
    name: evals/act
    needs: [run-build, determine-evals, run-e2e-bb-tests, run-regression-evals]
    if: >-
      ${{
        always() &&
        needs.run-build.result == 'success' &&
        needs.determine-evals.result == 'success' &&
        needs.run-e2e-bb-tests.result != 'failure' &&
        needs.run-e2e-bb-tests.result != 'cancelled' &&
        needs.run-regression-evals.result != 'failure' &&
        needs.run-regression-evals.result != 'cancelled' &&
        needs.determine-evals.outputs.skip-all-evals != 'true'
      }}
    runs-on: ubuntu-latest
    timeout-minutes: 25
    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      GOOGLE_GENERATIVE_AI_API_KEY: ${{ secrets.GOOGLE_GENERATIVE_AI_API_KEY }}
      BRAINTRUST_API_KEY: ${{ secrets.BRAINTRUST_API_KEY }}
      BROWSERBASE_API_KEY: ${{ secrets.BROWSERBASE_API_KEY }}
      BROWSERBASE_PROJECT_ID: ${{ secrets.BROWSERBASE_PROJECT_ID }}
      HEADLESS: true
      NODE_V8_COVERAGE: ${{ github.workspace }}/coverage/v8/evals/act
    steps:
      - name: Check out repository code
        uses: actions/checkout@v4

      - name: Check for 'act' label
        id: label-check
        run: |
          if [ "${{ needs.determine-evals.outputs.run-act }}" != "true" ]; then
            echo "has_label=false" >> $GITHUB_OUTPUT
            echo "No label for ACT. Exiting with success."
          else
            echo "has_label=true" >> $GITHUB_OUTPUT
          fi

      - uses: ./.github/actions/setup-node-pnpm-turbo
        if: needs.determine-evals.outputs.run-act == 'true'

      - name: Select Browserbase region
        if: needs.determine-evals.outputs.run-act == 'true'
        uses: ./.github/actions/select-browserbase-region
        with:
          distribution: ${{ env.BROWSERBASE_REGION_DISTRIBUTION }}

      - name: Run Act Evals
        if: needs.determine-evals.outputs.run-act == 'true'
        run: pnpm run evals category act

      - name: Log Act Evals Performance
        if: needs.determine-evals.outputs.run-act == 'true'
        run: |
          experimentName=$(jq -r '.experimentName' eval-summary.json)
          echo "View results at https://www.braintrust.dev/app/Browserbase/p/stagehand/experiments/${experimentName}"
          if [ -f eval-summary.json ]; then
            act_score=$(jq '.categories.act' eval-summary.json)
            echo "Act category score: $act_score%"
            if (( $(echo "$act_score < 80" | bc -l) )); then
              echo "Act category score is below 80%. Failing CI."
              exit 1
            fi
          else
            echo "Eval summary not found for act category. Failing CI."
            exit 1
          fi

      - uses: ./.github/actions/write-evals-ctrf
        if: always() && needs.determine-evals.outputs.run-act == 'true'
        with:
          category: act

      - uses: ./.github/actions/publish-ctrf-report
        if: always() && needs.determine-evals.outputs.run-act == 'true'
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - uses: ./.github/actions/upload-v8-coverage
        if: always() && needs.determine-evals.outputs.run-act == 'true'
        with:
          name: coverage-v8-evals-act

  run-extract-evals:
    name: evals/extract
    needs: [run-build, determine-evals, run-e2e-bb-tests, run-regression-evals]
    if: >-
      ${{
        always() &&
        needs.run-build.result == 'success' &&
        needs.determine-evals.result == 'success' &&
        needs.run-e2e-bb-tests.result != 'failure' &&
        needs.run-e2e-bb-tests.result != 'cancelled' &&
        needs.run-regression-evals.result != 'failure' &&
        needs.run-regression-evals.result != 'cancelled' &&
        needs.determine-evals.outputs.skip-all-evals != 'true'
      }}
    runs-on: ubuntu-latest
    timeout-minutes: 50
    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      GOOGLE_GENERATIVE_AI_API_KEY: ${{ secrets.GOOGLE_GENERATIVE_AI_API_KEY }}
      BRAINTRUST_API_KEY: ${{ secrets.BRAINTRUST_API_KEY }}
      BROWSERBASE_API_KEY: ${{ secrets.BROWSERBASE_API_KEY }}
      BROWSERBASE_PROJECT_ID: ${{ secrets.BROWSERBASE_PROJECT_ID }}
      HEADLESS: true
      NODE_V8_COVERAGE: ${{ github.workspace }}/coverage/v8/evals/extract
    steps:
      - name: Check out repository code
        uses: actions/checkout@v4

      - name: Check for 'extract' label
        id: label-check
        run: |
          if [ "${{ needs.determine-evals.outputs.run-extract }}" != "true" ]; then
            echo "has_label=false" >> $GITHUB_OUTPUT
            echo "No label for EXTRACT. Exiting with success."
          else
            echo "has_label=true" >> $GITHUB_OUTPUT
          fi

      - uses: ./.github/actions/setup-node-pnpm-turbo
        if: needs.determine-evals.outputs.run-extract == 'true'

      - name: Select Browserbase region
        if: needs.determine-evals.outputs.run-extract == 'true'
        uses: ./.github/actions/select-browserbase-region
        with:
          distribution: ${{ env.BROWSERBASE_REGION_DISTRIBUTION }}

      # 1. Run extract category with domExtract
      - name: Run Extract Evals (domExtract)
        if: needs.determine-evals.outputs.run-extract == 'true'
        run: pnpm run evals category extract -- --extract-method=domExtract

      - name: Save Extract Dom Results
        if: needs.determine-evals.outputs.run-extract == 'true'
        run: mv eval-summary.json eval-summary-extract-dom.json

      # 2. Log and Compare Extract Evals Performance
      - name: Log and Compare Extract Evals Performance
        if: needs.determine-evals.outputs.run-extract == 'true'
        run: |
          experimentNameDom=$(jq -r '.experimentName' eval-summary-extract-dom.json)
          dom_score=$(jq '.categories.extract' eval-summary-extract-dom.json)
          echo "DomExtract Extract category score: $dom_score%"
          echo "View domExtract results: https://www.braintrust.dev/app/Browserbase/p/stagehand/experiments/${experimentNameDom}"

          # If domExtract <80% fail CI
          if (( $(echo "$dom_score < 80" | bc -l) )); then
            echo "DomExtract extract category score is below 80%. Failing CI."
            exit 1
          fi

      - uses: ./.github/actions/write-evals-ctrf
        if: always() && needs.determine-evals.outputs.run-extract == 'true'
        with:
          category: extract
          summary-path: eval-summary-extract-dom.json

      - uses: ./.github/actions/publish-ctrf-report
        if: always() && needs.determine-evals.outputs.run-extract == 'true'
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - uses: ./.github/actions/upload-v8-coverage
        if: always() && needs.determine-evals.outputs.run-extract == 'true'
        with:
          name: coverage-v8-evals-extract

  run-observe-evals:
    name: evals/observe
    needs: [run-build, determine-evals, run-e2e-bb-tests, run-regression-evals]
    if: >-
      ${{
        always() &&
        needs.run-build.result == 'success' &&
        needs.determine-evals.result == 'success' &&
        needs.run-e2e-bb-tests.result != 'failure' &&
        needs.run-e2e-bb-tests.result != 'cancelled' &&
        needs.run-regression-evals.result != 'failure' &&
        needs.run-regression-evals.result != 'cancelled' &&
        needs.determine-evals.outputs.skip-all-evals != 'true'
      }}
    runs-on: ubuntu-latest
    timeout-minutes: 60
    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      GOOGLE_GENERATIVE_AI_API_KEY: ${{ secrets.GOOGLE_GENERATIVE_AI_API_KEY }}
      BRAINTRUST_API_KEY: ${{ secrets.BRAINTRUST_API_KEY }}
      BROWSERBASE_API_KEY: ${{ secrets.BROWSERBASE_API_KEY }}
      BROWSERBASE_PROJECT_ID: ${{ secrets.BROWSERBASE_PROJECT_ID }}
      HEADLESS: true
      NODE_V8_COVERAGE: ${{ github.workspace }}/coverage/v8/evals/observe
    steps:
      - name: Check out repository code
        uses: actions/checkout@v4

      - name: Check for 'observe' label
        id: label-check
        run: |
          if [ "${{ needs.determine-evals.outputs.run-observe }}" != "true" ]; then
            echo "has_label=false" >> $GITHUB_OUTPUT
            echo "No label for OBSERVE. Exiting with success."
          else
            echo "has_label=true" >> $GITHUB_OUTPUT
          fi

      - uses: ./.github/actions/setup-node-pnpm-turbo
        if: needs.determine-evals.outputs.run-observe == 'true'

      - name: Select Browserbase region
        if: needs.determine-evals.outputs.run-observe == 'true'
        uses: ./.github/actions/select-browserbase-region
        with:
          distribution: ${{ env.BROWSERBASE_REGION_DISTRIBUTION }}

      - name: Run Observe Evals
        if: needs.determine-evals.outputs.run-observe == 'true'
        run: pnpm run evals category observe

      - name: Log Observe Evals Performance
        if: needs.determine-evals.outputs.run-observe == 'true'
        run: |
          experimentName=$(jq -r '.experimentName' eval-summary.json)
          echo "View results at https://www.braintrust.dev/app/Browserbase/p/stagehand/experiments/${experimentName}"
          if [ -f eval-summary.json ]; then
            observe_score=$(jq '.categories.observe' eval-summary.json)
            echo "Observe category score: $observe_score%"
            if (( $(echo "$observe_score < 80" | bc -l) )); then
              echo "Observe category score is below 80%. Failing CI."
              exit 1
            fi
          else
            echo "Eval summary not found for observe category. Failing CI."
            exit 1
          fi

      - uses: ./.github/actions/write-evals-ctrf
        if: always() && needs.determine-evals.outputs.run-observe == 'true'
        with:
          category: observe

      - uses: ./.github/actions/publish-ctrf-report
        if: always() && needs.determine-evals.outputs.run-observe == 'true'
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - uses: ./.github/actions/upload-v8-coverage
        if: always() && needs.determine-evals.outputs.run-observe == 'true'
        with:
          name: coverage-v8-evals-observe

  run-targeted-extract-evals:
    name: evals/targeted-extract
    needs: [run-build, determine-evals, run-e2e-bb-tests, run-regression-evals]
    if: >-
      ${{
        always() &&
        needs.run-build.result == 'success' &&
        needs.determine-evals.result == 'success' &&
        needs.run-e2e-bb-tests.result != 'failure' &&
        needs.run-e2e-bb-tests.result != 'cancelled' &&
        needs.run-regression-evals.result != 'failure' &&
        needs.run-regression-evals.result != 'cancelled' &&
        needs.determine-evals.outputs.skip-all-evals != 'true'
      }}
    runs-on: ubuntu-latest
    timeout-minutes: 60
    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      GOOGLE_GENERATIVE_AI_API_KEY: ${{ secrets.GOOGLE_GENERATIVE_AI_API_KEY }}
      BRAINTRUST_API_KEY: ${{ secrets.BRAINTRUST_API_KEY }}
      BROWSERBASE_API_KEY: ${{ secrets.BROWSERBASE_API_KEY }}
      BROWSERBASE_PROJECT_ID: ${{ secrets.BROWSERBASE_PROJECT_ID }}
      HEADLESS: true
      NODE_V8_COVERAGE: ${{ github.workspace }}/coverage/v8/evals/targeted-extract
    steps:
      - name: Check out repository code
        uses: actions/checkout@v4

      - name: Check for 'targeted-extract' label
        id: label-check
        run: |
          if [ "${{ needs.determine-evals.outputs.run-targeted-extract }}" != "true" ]; then
            echo "has_label=false" >> $GITHUB_OUTPUT
            echo "No label for TARGETED-EXTRACT. Exiting with success."
          else
            echo "has_label=true" >> $GITHUB_OUTPUT
          fi

      - uses: ./.github/actions/setup-node-pnpm-turbo
        if: needs.determine-evals.outputs.run-targeted-extract == 'true'

      - name: Select Browserbase region
        if: needs.determine-evals.outputs.run-targeted-extract == 'true'
        uses: ./.github/actions/select-browserbase-region
        with:
          distribution: ${{ env.BROWSERBASE_REGION_DISTRIBUTION }}

      - name: Run targeted extract Evals
        if: needs.determine-evals.outputs.run-targeted-extract == 'true'
        run: pnpm run evals category targeted_extract

      - name: Log targeted extract Evals Performance
        if: needs.determine-evals.outputs.run-targeted-extract == 'true'
        run: |
          experimentName=$(jq -r '.experimentName' eval-summary.json)
          echo "View results at https://www.braintrust.dev/app/Browserbase/p/stagehand/experiments/${experimentName}"
          if [ -f eval-summary.json ]; then
            targeted_extract_score=$(jq '.categories.targeted_extract' eval-summary.json)
            echo "Targeted extract category score: $targeted_extract_score%"
            if (( $(echo "$targeted_extract_score < 80" | bc -l) )); then
              echo "Targeted extract score is below 80%. Failing CI."
              exit 1
            fi
          else
            echo "Eval summary not found for targeted_extract category. Failing CI."
            exit 1
          fi

      - uses: ./.github/actions/write-evals-ctrf
        if: always() && needs.determine-evals.outputs.run-targeted-extract == 'true'
        with:
          category: targeted_extract

      - uses: ./.github/actions/publish-ctrf-report
        if: always() && needs.determine-evals.outputs.run-targeted-extract == 'true'
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - uses: ./.github/actions/upload-v8-coverage
        if: always() && needs.determine-evals.outputs.run-targeted-extract == 'true'
        with:
          name: coverage-v8-evals-targeted-extract

  run-agent-evals:
    name: evals/agent
    needs: [run-build, determine-evals, run-e2e-bb-tests, run-regression-evals]
    if: >-
      ${{
        always() &&
        needs.run-build.result == 'success' &&
        needs.determine-evals.result == 'success' &&
        needs.run-e2e-bb-tests.result != 'failure' &&
        needs.run-e2e-bb-tests.result != 'cancelled' &&
        needs.run-regression-evals.result != 'failure' &&
        needs.run-regression-evals.result != 'cancelled' &&
        needs.determine-evals.outputs.skip-all-evals != 'true'
      }}
    runs-on: ubuntu-latest
    timeout-minutes: 90 # Agent evals can be long-running
    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      GOOGLE_GENERATIVE_AI_API_KEY: ${{ secrets.GOOGLE_GENERATIVE_AI_API_KEY }}
      BRAINTRUST_API_KEY: ${{ secrets.BRAINTRUST_API_KEY }}
      BROWSERBASE_API_KEY: ${{ secrets.BROWSERBASE_API_KEY }}
      BROWSERBASE_PROJECT_ID: ${{ secrets.BROWSERBASE_PROJECT_ID }}
      HEADLESS: true
      NODE_V8_COVERAGE: ${{ github.workspace }}/coverage/v8/evals/agent
    steps:
      - name: Check out repository code
        uses: actions/checkout@v4

      - name: Check for 'agent' label
        id: label-check
        run: |
          if [ "${{ needs.determine-evals.outputs.run-agent }}" != "true" ]; then
            echo "has_label=false" >> $GITHUB_OUTPUT
            echo "No label for AGENT. Exiting with success."
          else
            echo "has_label=true" >> $GITHUB_OUTPUT
          fi

      - uses: ./.github/actions/setup-node-pnpm-turbo
        if: needs.determine-evals.outputs.run-agent == 'true'

      - name: Select Browserbase region
        if: needs.determine-evals.outputs.run-agent == 'true'
        uses: ./.github/actions/select-browserbase-region
        with:
          distribution: ${{ env.BROWSERBASE_REGION_DISTRIBUTION }}

      - name: Run Agent Evals
        if: needs.determine-evals.outputs.run-agent == 'true'
        run: pnpm run evals category agent trials=${EVAL_AGENT_TRIAL_COUNT} concurrency=${EVAL_AGENT_MAX_CONCURRENCY}

      - name: Log Agent Evals Performance
        if: needs.determine-evals.outputs.run-agent == 'true'
        run: |
          experimentName=$(jq -r '.experimentName' eval-summary.json)
          echo "View results at https://www.braintrust.dev/app/Browserbase/p/stagehand/experiments/${experimentName}"
          if [ -f eval-summary.json ]; then
            agent_score=$(jq '.categories.agent' eval-summary.json)
            echo "Agent category score: $agent_score%"
            # Lower threshold for agent evals since they're complex
            if (( $(echo "$agent_score < 50" | bc -l) )); then
              echo "Agent category score is below 50%. Failing CI."
              exit 1
            fi
          else
            echo "Eval summary not found for agent category. Failing CI."
            exit 1
          fi

      - uses: ./.github/actions/write-evals-ctrf
        if: always() && needs.determine-evals.outputs.run-agent == 'true'
        with:
          category: agent

      - uses: ./.github/actions/publish-ctrf-report
        if: always() && needs.determine-evals.outputs.run-agent == 'true'
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - uses: ./.github/actions/upload-v8-coverage
        if: always() && needs.determine-evals.outputs.run-agent == 'true'
        with:
          name: coverage-v8-evals-agent

  merge-coverage:
    name: Merge coverage
    runs-on: ubuntu-latest
    needs:
      - core-unit-tests
      - run-e2e-local-tests
      - run-e2e-bb-tests
      - run-regression-evals
      - run-combination-evals
      - run-act-evals
      - run-extract-evals
      - run-observe-evals
      - run-targeted-extract-evals
      - run-agent-evals
      - server-unit-tests
      - server-integration-tests
    if: always()
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - uses: ./.github/actions/setup-node-pnpm-turbo

      - name: Download V8 coverage artifacts
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          pattern: coverage-v8-*
          path: coverage/v8
          merge-multiple: true

      - name: Generate merged coverage report
        run: |
          if [ ! -d coverage/v8 ] || [ -z "$(find coverage/v8 -type f -name '*.json' -print -quit)" ]; then
            echo "No V8 coverage files found."
            exit 0
          fi
          pnpm dlx c8 report \
            --temp-directory coverage/v8 \
            --reporter=lcov \
            --reporter=text-summary \
            --reports-dir coverage/merged \
            --include "packages/**" \
            --exclude "**/node_modules/**" \
            --exclude "packages/**/test/**" \
            --exclude "packages/**/tests/**" \
            --exclude "packages/**/lib/**/tests/**" \
            --exclude "**/*.d.ts"

      - name: Upload merged coverage report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-merged
          path: coverage/merged
