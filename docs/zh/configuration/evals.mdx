---
title: 评估与度量
sidebarTitle: 评估
description: 监控性能、优化成本，并评估 LLM 的效果
---

评估可帮助你了解自动化的实际表现、哪些模型最契合你的用例，以及如何在成本与可靠性之间实现最优平衡。本指南同时涵盖对自有工作流的监控与开展全面评估的方法。

## 为什么评估很重要

- **性能优化**：找出哪些模型与配置最适合你的特定自动化任务
- **成本控制**：跟踪 token 用量与推理时长，优化支出
- **可靠性**：衡量成功率并识别失败模式
- **模型选择**：在真实世界任务上比较不同 LLM，以做出更明智的决策

<Card
  title="实时模型对比"
  icon="scale-balanced"
  href="https://www.stagehand.dev/evals"
>
  在 [Stagehand 评估套件与仪表盘](https://www.stagehand.dev/evals) 上查看不同 LLM 的实时性能对比
</Card>

## 全面评估

评估可帮助你系统化地测试并改进自动化工作流。Stagehand 提供内置评估，以及用于创建自定义评估的工具。

<Tip>
要运行评估，你需要克隆 [Stagehand 仓库](https://github.com/browserbase/stagehand) 并执行 `npm install` 安装依赖。
</Tip>

我们提供两类评估：
1. 确定性评估（Deterministic Evals）——不依赖任何 LLM 推理，可重复且确定。
2. 基于 LLM 的评估（LLM-based Evals）——用于测试 Stagehand 的 AI 基元（primitives）的核心功能。

### 基于 LLM 的评估

<Tip>
要运行 LLM 评估，你需要一个 [Braintrust 账号](https://www.braintrust.dev/docs/)。
</Tip>

在 Stagehand 仓库中运行 `npm run evals` 即可启动基于 LLM 的评估。这将验证 Stagehand 中各 LLM 基元是否按预期工作。

评估分为以下几类：
1. Act 评估——测试 `act` 方法的功能。
2. Extract 评估——测试 `extract` 方法的功能。
3. Observe 评估——测试 `observe` 方法的功能。
4. 组合评估——联合测试 `act`、`extract` 与 `observe` 方法。

#### 配置与运行评估
你可以在 [`evals/tasks`](https://github.com/browserbase/stagehand/tree/main/evals/tasks) 查看具体评估项。各评估根据 [`evals/evals.config.json`](https://github.com/browserbase/stagehand/blob/main/evals/evals.config.json) 归类分组。可在 [`evals/taskConfig.ts`](https://github.com/browserbase/stagehand/blob/main/evals/taskConfig.ts) 指定要运行的模型及其他通用任务配置。

要运行指定评估，执行 `npm run evals <eval>`；要运行某一类别下的所有评估，执行 `npm run evals category <category>`。

#### 查看评估结果
![评估结果](/images/evals.png)

评估结果可在 Braintrust 上查看。运行 `npm run evals` 后，终端会打印一个 Braintrust 链接，通过该链接可查看对应评估的结果。

默认情况下，每个评估会在每个模型上运行五次。“Exact Match” 列显示评估命中完全一致结果的百分比；“Error Rate” 列显示评估报错的百分比。

你可以在 Braintrust 的 UI 中按模型/评估进行筛选，并在所有评估间聚合结果。

### 确定性评估

要运行确定性评估，只需在 Stagehand 仓库中执行 `npm run e2e`。这将验证 Stagehand 对 Playwright 的集成是否按预期工作。

这些测试位于 [`evals/deterministic`](https://github.com/browserbase/stagehand/tree/main/evals/deterministic)，会在 Browserbase 浏览器与本地无头 Chromium 浏览器上运行。

## 创建自定义评估

### 步骤指南

<Steps>
<Step title="创建评估文件">
在 `evals/tasks/your-eval.ts` 中新建文件：

```typescript
import { EvalTask } from '../types';

export const customEvalTask: EvalTask = {
  name: 'custom_task_name',
  description: '测试特定的自动化工作流',
  
  // 测试前置步骤
  setup: async ({ page }) => {
    await page.goto('https://example.com');
  },
  
  // 实际测试
  task: async ({ stagehand, page }) => {
    // 你的自动化逻辑
    await stagehand.act({ action: 'click the login button' });
    const result = await stagehand.extract({ 
      instruction: 'Get the user name',
      schema: { username: 'string' }
    });
    return result;
  },
  
  // 验证
  validate: (result, expected) => {
    return result.username === expected.username;
  },
  
  // 测试用例
  testCases: [
    {
      input: { /* 测试输入 */ },
      expected: { username: 'john_doe' }
    }
  ],
  
  // 评估指标
  scoring: {
    exactMatch: true,
    timeout: 30000,
    retries: 2
  }
};
```
</Step>

<Step title="添加到配置">
更新 `evals/evals.config.json`：

```json
{
  "categories": {
    "custom": ["custom_task_name"],
    "existing_category": ["custom_task_name"]
  }
}
```
</Step>

<Step title="运行评估">
```bash
# 测试你的自定义评估
npm run evals custom_task_name

# 运行整个 custom 分类
npm run evals category custom
```
</Step>
</Steps>


## 自定义评估最佳实践

<AccordionGroup>
<Accordion title="测试设计原则">
- **原子性**：每个测试仅验证一个特定能力
- **确定性**：测试应产生一致的结果
- **真实性**：采用真实场景与网站
- **可度量**：定义清晰的成功/失败标准
</Accordion>

<Accordion title="性能优化">
- **并行执行**：设计测试可相互独立运行
- **资源管理**：每个测试结束后做好清理
- **超时处理**：为各类操作设置合理超时
- **错误恢复**：优雅地处理失败与重试
</Accordion>

<Accordion title="数据质量">
- **基准真值**：建立可靠的期望结果
- **边界用例**：覆盖边界条件与错误场景
- **统计显著性**：多次运行以提升可靠性
- **版本控制**：持续跟踪测试用例的变更
</Accordion>
</AccordionGroup>

### 评估故障排查
<AccordionGroup>
<Accordion title="评估超时">
**症状**：测试因超时错误而失败

**解决方案**：
- 在 `taskConfig.ts` 中增大超时
- 使用更快的模型（Gemini 2.5 Flash、GPT-4o Mini）
- 优化测试场景，降低复杂度
- 检查与 LLM 提供商的网络连接
</Accordion>

<Accordion title="结果不一致">
**症状**：同一测试随机通过或失败

**解决方案**：
- 将 temperature 设为 0 以获得确定性输出
- 增加重复次数以获得统计显著性
- 对复杂任务使用更强的模型
- 检查动态网站内容是否影响测试
</Accordion>

<Accordion title="评估成本高">
**症状**：Token 使用量超出预算

**解决方案**：
- 使用更具性价比的模型（Gemini 2.0 Flash、GPT-4o Mini）
- 在初期测试中减少重复次数
- 聚焦于特定评估类别
- 使用本地浏览器环境以降低 Browserbase 成本
</Accordion>

<Accordion title="Braintrust 集成问题">
**症状**：结果未上传到仪表盘

**解决方案**：
- 检查 Braintrust API 密钥配置
- 验证网络连接
- 将 Braintrust SDK 更新至最新版本
- 在 Braintrust 仪表盘中检查项目权限
</Accordion>
</AccordionGroup>