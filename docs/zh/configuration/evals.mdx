---
title: 评估与度量
sidebarTitle: 评估
description: 监控性能、优化成本，并评估 LLM 的效果
---

评估能帮助你了解自动化的实际表现、找出最适合你用例的模型，并在成本与可靠性之间做出最优权衡。本指南同时涵盖对自有工作流的监控与完整评估的执行。

## 为什么评估至关重要

- **性能优化**：找出最适合你特定自动化任务的模型与配置
- **成本控制**：跟踪 Token 使用量和推理时长，优化开销
- **可靠性**：衡量成功率并定位失败模式
- **模型选择**：在真实场景中对比不同 LLM，做出更有依据的选择

<Card
  title="实时模型对比"
  icon="scale-balanced"
  href="https://www.stagehand.dev/evals"
>
  在 [Stagehand Evals Dashboard](https://www.stagehand.dev/evals) 上查看不同 LLM 的实时性能对比
</Card>

## 全面评测

评测可帮助你系统化地测试并改进自动化工作流。Stagehand 提供内置评测以及用于创建自定义评测的工具。

<Tip>
要运行评测，你需要克隆 [Stagehand 仓库](https://github.com/browserbase/stagehand) 并执行 `npm install` 安装依赖。
</Tip>

我们有三种评测类型：
1. **确定性评测** - 这些评测具有确定性，可在不进行任何 LLM 推理的情况下运行。
2. **基于 LLM 的评测** - 这些评测用于测试 Stagehand 的 AI 基元的底层功能。


### 基于 LLM 的评测

<Tip>
要运行 LLM 评测，你需要一个 [Braintrust 账户](https://www.braintrust.dev/docs/)。
</Tip>

要运行基于 LLM 的评测，你可以在 Stagehand 仓库中执行 `npm run evals`。这会测试 Stagehand 中 LLM 基元的功能，以确保其按预期工作。

评测分为四类：
1. **Act 评测** - 测试 `act` 方法的功能。
2. **Extract 评测** - 测试 `extract` 方法的功能。
3. **Observe 评测** - 测试 `observe` 方法的功能。
4. **组合评测** - 同时测试 `act`、`extract` 和 `observe` 方法的功能。

#### 配置与运行评测
你可以在 [`evals/tasks`](https://github.com/browserbase/stagehand/tree/main/evals/tasks) 查看具体评测。每个评测会根据 [`evals/evals.config.json`](https://github.com/browserbase/stagehand/blob/main/evals/evals.config.json) 被分组到对应类别。你可以在 [`evals/taskConfig.ts`](https://github.com/browserbase/stagehand/blob/main/evals/taskConfig.ts) 中指定要运行的模型和其他通用任务配置。

要运行特定评测，你可以执行 `npm run evals <eval>`，或使用 `npm run evals category <category>` 运行某一类别中的全部评测。


#### 查看评测结果
![评测结果](/images/evals.png)

评测结果可在 Braintrust 上查看。运行 `npm run evals` 时，终端会输出一个 Braintrust 链接，你可通过该链接查看某个特定评测的结果。

默认情况下，每个评测会在每个模型上运行五次。“Exact Match” 列显示评测答案完全匹配的百分比；“Error Rate” 列显示评测出错的百分比。

你可以在 Braintrust 的 UI 中按模型/评测进行筛选，并在所有评测中汇总结果。

### 确定性评测

要运行确定性评测，你只需在 Stagehand 仓库中执行 `npm run e2e`。这将测试 Stagehand 中 Playwright 的功能，以确保其按预期工作。

这些测试位于 [`evals/deterministic`](https://github.com/browserbase/stagehand/tree/main/evals/deterministic)，并会同时在 Browserbase 浏览器和本地无头 Chromium 浏览器上运行。

## 创建自定义评测

### 分步指南

<Steps>
<Step title="创建评测文件">
在 `evals/tasks/your-eval.ts` 中创建一个新文件：

```typescript
import { EvalTask } from '../types';

export const customEvalTask: EvalTask = {
  name: 'custom_task_name',
  description: '测试特定的自动化工作流',
  
  // 测试准备
  setup: async ({ page }) => {
    await page.goto('https://example.com');
  },
  
  // 实际测试
  task: async ({ stagehand, page }) => {
    // 你的自动化逻辑
    await stagehand.act({ action: 'click the login button' });
    const result = await stagehand.extract({ 
      instruction: 'Get the user name',
      schema: { username: 'string' }
    });
    return result;
  },
  
  // 校验
  validate: (result, expected) => {
    return result.username === expected.username;
  },
  
  // 测试用例
  testCases: [
    {
      input: { /* 测试输入 */ },
      expected: { username: 'john_doe' }
    }
  ],
  
  // 评估指标
  scoring: {
    exactMatch: true,
    timeout: 30000,
    retries: 2
  }
};
```
</Step>

<Step title="添加到配置">
更新 `evals/evals.config.json`：

```json
{
  "categories": {
    "custom": ["custom_task_name"],
    "existing_category": ["custom_task_name"]
  }
}
```
</Step>

<Step title="运行自定义评测">
```bash
# 测试你的自定义评测
npm run evals custom_task_name

# 运行整个 custom 分类
npm run evals category custom
```
</Step>
</Steps>


## 自定义评测的最佳实践

<AccordionGroup>
<Accordion title="测试设计原则">
- **原子性**：每个测试应只验证一个具体能力
- **确定性**：测试应产生一致的结果
- **贴近现实**：使用真实世界的场景与网站
- **可度量**：定义清晰的成功/失败标准
</Accordion>

<Accordion title="性能优化">
- **并行执行**：将测试设计为可独立运行
- **资源管理**：每个测试后进行清理
- **超时处理**：为各类操作设置合适的超时时间
- **错误恢复**：平滑地处理失败并恢复
</Accordion>

<Accordion title="数据质量">
- **标准答案（Ground Truth）**：建立可靠的预期结果
- **边界情况**：覆盖边界条件与错误场景
- **统计显著性**：多次迭代以提高可靠性
- **版本控制**：持续追踪测试用例的变更
</Accordion>
</AccordionGroup>

### 评测故障排查
<AccordionGroup>
<Accordion title="评测超时">
**症状**：测试因超时错误而失败

**解决方案**：
- 在 `taskConfig.ts` 中增加超时时间
- 使用更快的模型（Gemini 2.5 Flash、GPT-4o Mini）
- 简化测试场景以降低复杂度
- 检查与 LLM 提供商的网络连接
</Accordion>

<Accordion title="结果不一致">
**症状**：同一测试随机通过/失败

**解决方案**：
- 将 temperature 设为 0 以获得确定性输出
- 增加重复次数以提高统计显著性
- 对复杂任务使用更强的模型
- 检查网站动态内容是否影响测试
</Accordion>

<Accordion title="评测成本高">
**症状**：Token 使用量超出预算

**解决方案**：
- 使用更具性价比的模型（Gemini 2.0 Flash、GPT-4o Mini）
- 初期测试时减少重复次数
- 聚焦于特定评测类别
- 使用本地浏览器环境以降低 Browserbase 成本
</Accordion>

<Accordion title="Braintrust 集成问题">
**症状**：结果未上传到控制台

**解决方案**：
- 检查 Braintrust API Key 配置
- 验证网络连接
- 将 Braintrust SDK 更新到最新版本
- 在 Braintrust 控制台检查项目权限
</Accordion>
</AccordionGroup>