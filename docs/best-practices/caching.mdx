---
title: Caching Actions
description: Speed up workflows by caching Stagehand actions and avoiding redundant LLM calls.
---

Caching actions in Stagehand saves time and money by reusing previously determined actions. Use caching for:
- Frequently repeated actions
- Pages with stable DOM structure
- Multi-step workflows with predictable patterns

## How Caching Works: Step by Step

### Step 1: Get Action from LLM
When you call `observe()`, Stagehand sends your prompt to the LLM and gets back structured action data:

<CodeGroup>
```typescript TypeScript
// INPUT: Your natural language prompt
const [action] = await page.observe("Click the login button");

// OUTPUT: Structured action object that looks like:
console.log(action);
/*
{
  "selector": "button[data-testid='login-btn']",
  "method": "click", 
  "arguments": [],
  "description": "Login button in the header"
}
*/
```

```python Python
# INPUT: Your natural language prompt  
actions = await page.observe("Click the login button")
action = actions[0]

# OUTPUT: Structured action dictionary that looks like:
print(action)
"""
{
  "selector": "button[data-testid='login-btn']",
  "method": "click",
  "arguments": [],
  "description": "Login button in the header"  
}
"""
```
</CodeGroup>

### Step 2: Store Action in Cache
Take that action object and save it with a unique key:

<CodeGroup>
```typescript TypeScript
// STORAGE: Save action object to your cache system
cache.set("login-action", action);
// ‚Üí Stored in memory/file/Redis with key "login-action"
```

```python Python
# STORAGE: Save action object to your cache system
cache.set("login-action", action)  
# ‚Üí Stored in memory/file/Redis with key "login-action"
```
</CodeGroup>

### Step 3: Reuse Cached Action
Later, retrieve and execute the cached action without calling the LLM:

<CodeGroup>
```typescript TypeScript
// RETRIEVAL: Get cached action (no LLM call needed)
const cachedAction = cache.get("login-action");

// EXECUTION: Use the cached selector and method directly
await page.act(cachedAction);
// ‚Üí Clicks button[data-testid='login-btn'] immediately
```

```python Python
# RETRIEVAL: Get cached action (no LLM call needed)
cached_action = cache.get("login-action")

# EXECUTION: Use the cached selector and method directly  
await page.act(cached_action)
# ‚Üí Clicks button[data-testid='login-btn'] immediately
```
</CodeGroup>

## File-Based Cache Implementation

Here's a complete working example that saves actions to a local JSON file:

### What the cache file looks like

After running some cached actions, your `cache.json` file will contain:

```json
{
  "Click the login button": {
    "selector": "button[data-testid='login-btn']",
    "method": "click",
    "arguments": [],
    "description": "Login button in the header"
  },
  "Fill username field": {
    "selector": "input[name='username']", 
    "method": "fill",
    "arguments": ["john@example.com"],
    "description": "Username input field"
  }
}
```

### Implementation code

<CodeGroup>
```typescript TypeScript
import { readFile, writeFile } from 'fs/promises';

// READ from cache.json file ‚Üí return action object or null
async function getCache(key: string) {
  try {
    const data = await readFile('cache.json', 'utf8');
    const cache = JSON.parse(data);
    return cache[key]; // Returns the action object
  } catch {
    return null; // File doesn't exist or invalid JSON
  }
}

// WRITE action object ‚Üí cache.json file  
async function setCache(key: string, value: any) {
  let cache = {};
  try {
    // Load existing cache first
    const data = await readFile('cache.json', 'utf8');
    cache = JSON.parse(data);
  } catch {
    // Start with empty cache if file doesn't exist
  }
  
  // Add new action to cache
  cache[key] = value;
  
  // Save entire cache back to file
  await writeFile('cache.json', JSON.stringify(cache, null, 2));
  console.log(`‚úÖ Cached action: "${key}"`);
}

// MAIN FUNCTION: Check cache first, then observe if needed
async function actWithCache(page, prompt: string, key?: string) {
  const cacheKey = key || prompt;
  
  // 1. CHECK CACHE: Look for existing action
  let action = await getCache(cacheKey);
  
  if (!action) {
    // 2. CACHE MISS: Get action from LLM and store it
    console.log(`üîç Cache miss for "${cacheKey}" - calling LLM...`);
    [action] = await page.observe(prompt);
    await setCache(cacheKey, action);
  } else {
    // 3. CACHE HIT: Use stored action 
    console.log(`‚ö° Cache hit for "${cacheKey}" - no LLM call needed!`);
  }
  
  // 4. EXECUTE: Run the action (cached or fresh)
  try {
    await page.act(action);
    console.log(`‚úÖ Action executed: ${action.method} on ${action.selector}`);
  } catch (error) {
    // 5. SELF-HEAL: If cached action fails, try fresh LLM call
    console.log(`‚ùå Cached action failed, self-healing with LLM...`);
    await page.act(prompt);
  }
}
```

```python Python
import json
import aiofiles
from typing import Optional, Dict, Any

# READ from cache.json file ‚Üí return action dict or None
async def get_cache(key: str) -> Optional[Dict[str, Any]]:
    try:
        async with aiofiles.open('cache.json', 'r') as f:
            data = await f.read()
            cache = json.loads(data)
            return cache.get(key)  # Returns the action dict
    except (FileNotFoundError, json.JSONDecodeError):
        return None  # File doesn't exist or invalid JSON

# WRITE action dict ‚Üí cache.json file
async def set_cache(key: str, value: Dict[str, Any]) -> None:
    cache = {}
    try:
        # Load existing cache first
        async with aiofiles.open('cache.json', 'r') as f:
            data = await f.read()
            cache = json.loads(data)
    except (FileNotFoundError, json.JSONDecodeError):
        pass  # Start with empty cache if file doesn't exist
    
    # Add new action to cache
    cache[key] = value
    
    # Save entire cache back to file
    async with aiofiles.open('cache.json', 'w') as f:
        await f.write(json.dumps(cache, indent=2))
    print(f"‚úÖ Cached action: \"{key}\"")

# MAIN FUNCTION: Check cache first, then observe if needed
async def act_with_cache(page, prompt: str, key: str = None):
    cache_key = key or prompt
    
    # 1. CHECK CACHE: Look for existing action
    action = await get_cache(cache_key)
    
    if not action:
        # 2. CACHE MISS: Get action from LLM and store it
        print(f"üîç Cache miss for \"{cache_key}\" - calling LLM...")
        actions = await page.observe(prompt)
        action = actions[0]
        await set_cache(cache_key, action)
    else:
        # 3. CACHE HIT: Use stored action
        print(f"‚ö° Cache hit for \"{cache_key}\" - no LLM call needed!")
    
    # 4. EXECUTE: Run the action (cached or fresh)
    try:
        await page.act(action)
        print(f"‚úÖ Action executed: {action['method']} on {action['selector']}")
    except Exception as error:
        # 5. SELF-HEAL: If cached action fails, try fresh LLM call
        print("‚ùå Cached action failed, self-healing with LLM...")
        await page.act(prompt)
```
</CodeGroup>

### Complete Example: See the Flow in Action

<CodeGroup>
```typescript TypeScript
// First time running this action
await actWithCache(page, "Click the login button");
/*
Console output:
üîç Cache miss for "Click the login button" - calling LLM...
‚úÖ Cached action: "Click the login button"  
‚úÖ Action executed: click on button[data-testid='login-btn']
*/

// Second time - same action uses cache
await actWithCache(page, "Click the login button");
/*
Console output:
‚ö° Cache hit for "Click the login button" - no LLM call needed!
‚úÖ Action executed: click on button[data-testid='login-btn']
*/

// Different action - cache miss again
await actWithCache(page, "Fill username field", "username-input");
/*
Console output:
üîç Cache miss for "username-input" - calling LLM...
‚úÖ Cached action: "username-input"
‚úÖ Action executed: fill on input[name='username']
*/
```

```python Python
# First time running this action
await act_with_cache(page, "Click the login button")
"""
Console output:
üîç Cache miss for "Click the login button" - calling LLM...
‚úÖ Cached action: "Click the login button"  
‚úÖ Action executed: click on button[data-testid='login-btn']
"""

# Second time - same action uses cache
await act_with_cache(page, "Click the login button")
"""
Console output:
‚ö° Cache hit for "Click the login button" - no LLM call needed!
‚úÖ Action executed: click on button[data-testid='login-btn']  
"""

# Different action - cache miss again
await act_with_cache(page, "Fill username field", "username-input")
"""
Console output:
üîç Cache miss for "username-input" - calling LLM...
‚úÖ Cached action: "username-input"
‚úÖ Action executed: fill on input[name='username']
"""
```
</CodeGroup>

After running these examples, your `cache.json` file will look like:

```json
{
  "Click the login button": {
    "selector": "button[data-testid='login-btn']",
    "method": "click",
    "arguments": [],
    "description": "Login button in the header"
  },
  "username-input": {
    "selector": "input[name='username']",
    "method": "fill", 
    "arguments": [""],
    "description": "Username input field"
  }
}
```

## Best Practices

### Smart Cache Keys
Create unique cache keys based on page content for better accuracy:

<CodeGroup>
```typescript TypeScript
// Use URL + element selector for more specific caching
const url = page.url();
const action = await getCache(`${url}:click-login`);

// Or hash page content for content-aware caching
const pageContent = await page.content();
const contentHash = createHash('md5').update(pageContent).digest('hex');
const key = `${contentHash}:login-action`;
```

```python Python
# Use URL + element selector for more specific caching  
url = page.url
action = await get_cache(f"{url}:click-login")

# Or hash page content for content-aware caching
import hashlib
page_content = await page.content()
content_hash = hashlib.md5(page_content.encode()).hexdigest()
key = f"{content_hash}:login-action"
```
</CodeGroup>

### Cache Management
- **TTL (Time To Live)**: Add timestamps to expire old actions
- **Invalidation**: Clear cache when page structure changes  
- **Size limits**: Prevent cache from growing too large
- **Environment-specific**: Use different caches for dev/staging/prod

### When to Use Caching
‚úÖ **Good for caching:**
- Stable UI elements (login buttons, navigation)
- Repeated actions in loops
- Form fields that don't change

‚ùå **Avoid caching:**
- Dynamic content (search results, user-generated content)
- Time-sensitive actions (live data, real-time updates)
- One-time actions

<Note>
Always implement self-healing (fallback to fresh LLM call) when cached actions fail due to page changes.
</Note>