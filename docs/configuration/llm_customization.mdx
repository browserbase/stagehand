---
title: LLM Customization
sidebarTitle: LLM Customization
description: Configure Stagehand to use different LLM providers and models for optimal performance, cost, and reliability
---

Stagehand uses Large Language Models (LLMs) to understand web pages, plan actions, and interact with complex interfaces. The choice of LLM significantly impacts your automation's accuracy, speed, and cost.

<Card title="Model Evaluation" href="https://www.stagehand.dev/evals" icon="paper-plane">
Find more details about how to choose the right model on our Model Evaluation page.
</Card>

## Why LLM Choice Matters

- **Accuracy**: Better models provide more reliable element detection and action planning
- **Speed**: Faster models reduce automation latency
- **Cost**: Different providers offer varying pricing structures
- **Reliability**: Structured output support ensures consistent automation behavior

<Tip>
**Recommended**: Start with **Google Gemini 2.5 Pro** for best results, then optimize for speed/cost based on your needs. Find more details about how to choose the right model on our [Model Evaluation](https://www.stagehand.dev/evals) page.
</Tip>

<Warning>
Small models on **Ollama** struggle with consistent structured outputs. While technically supported, we don't recommend them for production Stagehand workflows.
</Warning>

## Environment Variables Setup

Set up your API keys before configuring Stagehand:

<CodeGroup>
```bash .env
# Choose one or more providers
OPENAI_API_KEY=your_openai_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here
GOOGLE_API_KEY=your_google_key_here
GROQ_API_KEY=your_groq_key_here
```
</CodeGroup>

## Supported Providers

Stagehand supports major LLM providers with structured output capabilities:

### Production-Ready Providers

| Provider | Best Models | Strengths | Use Case |
|----------|-------------|-----------|----------|
| **OpenAI** | `gpt-4o`, `gpt-4o-mini` | High accuracy, reliable | Production, complex sites |
| **Anthropic** | `claude-3-7-sonnet-latest` | Excellent reasoning | Complex automation tasks |
| **Google** | `gemini-2.0-flash`, `gemini-2.5-pro` | Fast, cost-effective | High-volume automation |

### Additional Providers

<Expandable title="More Providers">
- **Groq** - `llama-3.3-70b-versatile` (Good for speed critical applications)
- **xAI** - `grok-beta` (Good for complex reasoning)
- **Azure** - Enterprise OpenAI deployment
- **Cerebras** - High-speed inference
- **TogetherAI** - Open-source models
- **Mistral** - `mixtral-8x7b-32768` (European option)
- **DeepSeek** - Cost-effective alternative
- **Perplexity** - Real-time web data
- **Ollama** - Local deployment (limited accuracy)
- **Run any model included in AI SDK** - Find supported models in the [Vercel AI SDK](https://sdk.vercel.ai/providers/ai-sdk-providers)
</Expandable>

## Basic Configuration

### Model Name Format

Stagehand uses the format `provider/model-name` for model specification.

**Examples:**
- OpenAI: `openai/gpt-4o`
- Anthropic: `anthropic/claude-3-7-sonnet-latest`
- Google: `google/gemini-2.5-pro`

### Quick Start Examples

<Tabs>
<Tab title="Google (Recommended)">
```typescript TypeScript
import { Stagehand } from "@browserbasehq/stagehand";

const stagehand = new Stagehand({
  modelName: "google/gemini-2.5-pro",
  modelClientOptions: {
    apiKey: process.env.GOOGLE_API_KEY,
  },
});
```
```python Python
import os
from stagehand import Stagehand

stagehand = Stagehand(
    model_name="google/gemini-2.5-pro",
    model_client_options={"api_key": os.getenv("GOOGLE_API_KEY")},
)
```
</Tab>
<Tab title="OpenAI">
```typescript TypeScript
import { Stagehand } from "@browserbasehq/stagehand";

const stagehand = new Stagehand({
  modelName: "openai/gpt-4o",
  modelClientOptions: {
    apiKey: process.env.OPENAI_API_KEY,
  },
});
```
```python Python
import os
from stagehand import Stagehand

stagehand = Stagehand(
    model_name="openai/gpt-4o",
    model_client_options={"api_key": os.getenv("OPENAI_API_KEY")},
)
```
</Tab>

<Tab title="Anthropic">
```typescript TypeScript
import { Stagehand } from "@browserbasehq/stagehand";

const stagehand = new Stagehand({
  modelName: "anthropic/claude-3-7-sonnet-latest",
  modelClientOptions: {
    apiKey: process.env.ANTHROPIC_API_KEY,
  },
});
```

```python Python
import os
from stagehand import Stagehand

stagehand = Stagehand(
    model_name="anthropic/claude-3-7-sonnet-latest",
    model_client_options={"api_key": os.getenv("ANTHROPIC_API_KEY")},
)
```
</Tab>
</Tabs>

## Custom LLM Integration

<Note>
Custom LLMs are currently only supported in TypeScript.
</Note>

Integrate any LLM with Stagehand using custom clients. The only requirement is **structured output support** for consistent automation behavior.

### OpenAI-compatible APIs
Most LLMs are OpenAI-compatible, and thus can be used with Stagehand as long as they support structured outputs. This includes models like Google Gemini, Ollama, and most Llama models including Groq, Cerebras, and more.

To get started, you can use the [OpenAI external client](https://github.com/browserbase/stagehand/blob/main/examples/external_clients/customOpenAI.ts) as a template to create a client for your model.
```ts {7-13}
import { Stagehand } from "@browserbasehq/stagehand";
import { CustomOpenAIClient } from "./external_clients/customOpenAI";
import OpenAI from "openai";

const stagehand = new Stagehand({
	...StagehandConfig,
	llmClient: new CustomOpenAIClient({
		modelName: "llama3.3",
		client: new OpenAI({
			apiKey: "ollama",
			baseURL: "http://localhost:11434/v1",
		}),
	}),
});

await stagehand.init();
```

### Vercel AI SDK
The [Vercel AI SDK](https://sdk.vercel.ai/providers/ai-sdk-providers) is a popular library for interacting with LLMs. You can use any of the providers supported by the Vercel AI SDK to create a client for your model, **as long as they support structured outputs**.

Vercel AI SDK supports providers for OpenAI, Anthropic, and Google, along with support for **Amazon Bedrock** and **Azure OpenAI**.

To get started, you'll need to install the `ai` package and the provider you want to use. For example, to use Amazon Bedrock, you'll need to install the `@ai-sdk/amazon-bedrock` package.

You'll also need to use the [Vercel AI SDK external client](https://github.com/browserbase/stagehand/blob/main/examples/external_clients/aisdk.ts) as a template to create a client for your model.

<Tabs>
	<Tab title="npm">
	```bash
	npm install ai @ai-sdk/amazon-bedrock
	```
	</Tab>

	<Tab title="pnpm">
	```bash
	pnpm install ai @ai-sdk/amazon-bedrock
	```
	</Tab>

	<Tab title="yarn">
	```bash
	yarn add ai @ai-sdk/amazon-bedrock
	```
	</Tab>
</Tabs>

To get started, you can use the [Vercel AI SDK external client](https://github.com/browserbase/stagehand/blob/84f810b4631291307a32a47addad7e26e9c1deb3/examples/external_clients/aisdk.ts) as a template to create a client for your model.

```ts
// Install/import the provider you want to use.
// For example, to use OpenAI, import `openai` from @ai-sdk/openai
import { bedrock } from "@ai-sdk/amazon-bedrock";
import { AISdkClient } from "./external_clients/aisdk";

const stagehand = new Stagehand({
  llmClient: new AISdkClient({
	model: bedrock("anthropic.claude-3-7-sonnet-20250219-v1:0"),
  }),
});
```

## Troubleshooting

### Common Issues

<AccordionGroup>
<Accordion title="Model doesn't support structured outputs">
**Error**: `Model does not support structured outputs`

**Solution**: 
Use models that support function calling/structured outputs:
- **OpenAI**: GPT-4o, GPT-4o-mini, GPT-4-turbo
- **Anthropic**: Claude 3.7 Sonnet Latest, Claude 3.5 Sonnet, Claude 3 Haiku
- **Google**: Gemini 2.5 Pro, Gemini 2.0 Flash, Gemini 1.5 Pro
- **Groq**: Llama 3.3 70B Versatile

**Note**: Avoid older models like GPT-3.5-turbo or base Llama models without structured output fine-tuning.
</Accordion>

<Accordion title="Authentication errors">
**Error**: `Invalid API key` or `Unauthorized`

**Solution**:
- Verify your environment variables are set correctly
- Check API key permissions and quotas
- Ensure you're using the correct API key for the provider
- For Anthropic, make sure you have access to the Claude API
</Accordion>

<Accordion title="Inconsistent automation results">
**Symptoms**: Actions work sometimes but fail other times

**Causes & Solutions**:
- **Weak models**: Use more capable models - **recommended: Gemini 2.5 Pro**, GPT-4o, or Claude 3.7 Sonnet Latest
- **High temperature**: Set temperature to 0 for deterministic outputs
- **Complex pages**: Switch to most capable models like GPT-4o or Claude 3.7 Sonnet Latest
- **Rate limits**: Implement retry logic with exponential backoff
- **Context limits**: Reduce page complexity or use models with larger context windows
- **Prompt clarity**: Ensure your automation instructions are clear and specific
</Accordion>

<Accordion title="Slow performance">
**Issue**: Automation takes too long to respond

**Solutions**:
- **Switch to faster models**: 
  - **Google Gemini 2.0 Flash** (good speed/accuracy balance)
  - **GPT-4o-mini** (faster than GPT-4o)
- **Optimize settings**: 
  - Use `verbose: 0` to minimize token usage
  - Set lower temperature (0) for faster processing
  - Reduce max tokens if possible
- **Local deployment**: Use Ollama for zero-latency (with accuracy trade-offs)
- **Batch operations**: Group multiple actions when possible
</Accordion>

<Accordion title="High costs">
**Issue**: LLM usage costs are too high

**Cost Optimization Strategies**:
1. **Switch to cost-effective models**: 
   - **Gemini 2.5 Pro** (best value for high capability)
   - **Gemini 2.0 Flash** (ultra cost-effective)
   - **GPT-4o-mini** (cheaper than GPT-4o)
2. **Optimize token usage**: 
   - Set `verbose: 0` to reduce logging overhead
   - Use concise prompts and limit response length
3. **Smart model selection**: Start with cheaper models, fallback to premium ones only when needed
4. **Cache responses**: Implement LLM response caching for repeated automation patterns
5. **Monitor usage**: Set up billing alerts and track costs per automation run
6. **Batch processing**: Process multiple similar tasks together
</Accordion>
</AccordionGroup>

### Best Practices

1. **Start with recommended models**: Gemini 2.5 Pro for best results
2. **Test thoroughly**: Different models excel at different types of websites and complexity levels
3. **Monitor costs**: Set up billing alerts and track token usage per automation run
4. **Use appropriate temperature**: 0 for deterministic automation, 0.1-0.3 for creative tasks
5. **Implement fallbacks**: Have backup models in case your primary model fails or hits rate limits
6. **Cache strategically**: Store successful action patterns to reduce redundant LLM calls
7. **Version control**: Track which model versions work best for your specific use cases
8. **A/B testing**: Compare model performance on representative automation tasks

